<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=9" >

<title>Workflow</title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>



<!-- MathJax scripts -->
<script type="text/javascript" src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



</head>

<body>
<h1>Workflow</h1>

<p>We are using dataset provided by <a href="http://lshtc.iit.demokritos.gr/">lshtc</a> for their first competition, we will be working with their dry run dataset
which is a smaller compared to the actual one they use in the competition. In this dataset every document is represented as a feature vector, with features not occuring omitted. We are using slightly modified k nearest neighbour algorithm to classify the documents. Initially we used k in each hierarchical level to be <span style="white-space: nowrap; font-size: larger">
&radic;<span style="text-decoration:overline;">&nbsp;n/2 &nbsp;</span>. </p>

<p>In the current model we are using validation to find the value of the parameter \( k \) , since we are already given different validation sets apart from training set there is no need to use cross validation.</p>

<p>Lastly we are using PCA to reduce the number of dimensions within levels. Within each level there would be few dimensions which would not contribute to the classification and hence they are removed.</p>

<h2>K-Nearest Neighbor Algorithm</h2>

<p>This is one of the simplest and most intuitive Machine Learning algorithms there is for classification. The rationale is as follows:-
 When a test data point is given we try to imagine the &ldquo;smallest&rdquo; hyper-sphere centered at this test point such that it includes the nearest <em>k</em> training data points. Now we conduct a majority vote and our target point is assigned the class which majority of the included trainining points belong to.</p>

<p>To avoid a no-decision situation due to an equi-partition of votes we keep <em>k</em> odd when # of classes is even, and even otherwise. 
One can also vary the bias of the model by changing the parameter \( k \). When \( k = n \) it is a highly biased model with low variance. On the other hand when \( k \) = 1 the model has high variance and low bias. Hence \( k \)-NN is a very flexible model.
Number of data points in training set = \( n \).</p>

<p>Some of the benefits of \( k \)-NN which made it the major classifier for this project are</p>

<ul>
<li>Simple and easy to interpret</li>
<li>very flexible</li>
<li>is a type of lazy learning</li>
<li>has theoretical backing that when \( n \) is large the misclassification rate will be at-most twice the bayesian misclassification rate.</li>
</ul>

<p>The last benefit ensures that if # of points in training set is very large the \( k \)-NN classifier will perform very good.</p>

<p>The disadvantages of \( k \)-NN are</p>

<ul>
<li>requires high computational power when \( n \) gets large</li>
<li>when \( d \) the # of dimensions grows large Euclidean distance forces the hyper-sphere to grow very large. This phenomen is called <a href="http://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a></li>
<li>needs to keep all \( n \) points in memory.</li>
</ul>

<p>The error measure we are using is a simple error rate, which can be found out like this, when our training observations are given like this {(\( x_1, y_1 \)), &hellip; ,(\( x_n, y_n \))}
\( \frac{1}{n}\sum\limits_{i=1}^{n} I(\hat{y_{i}}\neq y_{i}) \)</p>

<p>where \( I(\hat{y_{i}}\neq y_{i}) \) is an indicator variable that equals one when \( y_{i} \neq \hat y_{i} \)
and \( \hat{y_{i}} \) is the predicted value for the obsevation \( x_{i} \) and it&#39;s corresponding true value/response = \( y_{i} \)</p>

</body>

</html>

